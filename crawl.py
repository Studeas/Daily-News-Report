from newsplease import NewsPlease
import json
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import urljoin, urlparse
import time
import os

LIMIT_NUM = 25

# æ–°é—»å¹³å°é¦–é¡µåˆ—è¡¨
homepage_urls = [
    'https://punchng.com/',
    'https://dailypost.ng/',
    'https://dailytrust.com/',
    'https://thesun.ng/',
    # 'https://guardian.ng/'
]

# è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®ä»¥é¿å… 403 é”™è¯¯
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Accept-Encoding': 'gzip, deflate',
    'Connection': 'keep-alive',
}

request_args = {'headers': headers}

def extract_article_links(homepage_url, soup):
    """
    ä»é¦–é¡µHTMLä¸­æå–æ–°é—»æ–‡ç« é“¾æ¥å’Œæ ‡é¢˜
    è¿™ä¸ªæ–¹æ³•éœ€è¦æ ¹æ®æ¯ä¸ªç½‘ç«™çš„å…·ä½“HTMLç»“æ„æ¥è°ƒæ•´
    """
    article_links = []
    base_domain = urlparse(homepage_url).netloc
    
    # å¸¸è§çš„æ–°é—»é“¾æ¥é€‰æ‹©å™¨ï¼ˆéœ€è¦æ ¹æ®å®é™…ç½‘ç«™è°ƒæ•´ï¼‰
    # å°è¯•å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨
    selectors = [
        'article a',           # æ–‡ç« æ ‡ç­¾å†…çš„é“¾æ¥
        '.post a',             # æ–‡ç« ç±»å†…çš„é“¾æ¥
        '.article a',          # articleç±»å†…çš„é“¾æ¥
        'h2 a', 'h3 a',        # æ ‡é¢˜å†…çš„é“¾æ¥
        '.entry-title a',      # æ ‡é¢˜ç±»å†…çš„é“¾æ¥
        '.news-item a',        # æ–°é—»é¡¹å†…çš„é“¾æ¥
        'a[href*="/article/"]',  # åŒ…å«/article/çš„é“¾æ¥
        'a[href*="/news/"]',      # åŒ…å«/news/çš„é“¾æ¥
        'a[href*="/story/"]',     # åŒ…å«/story/çš„é“¾æ¥
    ]
    
    found_links = set()  # ç”¨äºå»é‡
    
    for selector in selectors:
        links = soup.select(selector)
        for link in links:
            href = link.get('href')
            if not href:
                continue
            
            # è½¬æ¢ä¸ºç»å¯¹URL
            full_url = urljoin(homepage_url, href)
            parsed = urlparse(full_url)
            
            # è¿‡æ»¤æ¡ä»¶ï¼šåªä¿ç•™åŒåŸŸåçš„é“¾æ¥ï¼Œæ’é™¤é¦–é¡µã€åˆ†ç±»é¡µç­‰
            if (parsed.netloc == base_domain and 
                full_url not in found_links and
                full_url != homepage_url and
                not any(x in full_url.lower() for x in ['/category/', '/tag/', '/author/', '/page/', '/archive/'])):
                
                title = link.get_text(strip=True)
                if title and len(title) > 10:  # æ ‡é¢˜é•¿åº¦è¿‡æ»¤
                    found_links.add(full_url)
                    article_links.append({
                        'title': title,
                        'url': full_url,
                        'source': homepage_url
                    })
    
    # å¦‚æœä¸Šé¢çš„é€‰æ‹©å™¨æ²¡æ‰¾åˆ°ï¼Œå°è¯•æ›´é€šç”¨çš„æ–¹æ³•
    if not article_links:
        # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥ï¼Œè¿‡æ»¤å‡ºå¯èƒ½æ˜¯æ–‡ç« çš„é“¾æ¥
        all_links = soup.find_all('a', href=True)
        for link in all_links:
            href = link.get('href')
            full_url = urljoin(homepage_url, href)
            parsed = urlparse(full_url)
            
            if (parsed.netloc == base_domain and 
                full_url not in found_links and
                full_url != homepage_url and
                len(full_url) > len(homepage_url) + 10 and  # URLé•¿åº¦è¿‡æ»¤
                not any(x in full_url.lower() for x in ['/category/', '/tag/', '/author/', '/page/', '/archive/', '#'])):
                
                title = link.get_text(strip=True)
                if title and len(title) > 10:
                    found_links.add(full_url)
                    article_links.append({
                        'title': title,
                        'url': full_url,
                        'source': homepage_url
                    })
    
    # å»é‡å¹¶é™åˆ¶æ•°é‡
    seen = set()
    unique_links = []
    for item in article_links:
        if item['url'] not in seen:
            seen.add(item['url'])
            unique_links.append(item)
            if len(unique_links) >= LIMIT_NUM:
                break
    
    return unique_links

def serialize_article(article):
    """å°†æ–‡ç« å¯¹è±¡æˆ–å­—å…¸è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸"""
    if article is None:
        return None
    
    # å¦‚æœå·²ç»æ˜¯å­—å…¸ï¼Œç›´æ¥ä½¿ç”¨
    if isinstance(article, dict):
        data = article
    # å¦‚æœæ˜¯å¯¹è±¡ï¼Œè½¬æ¢ä¸ºå­—å…¸
    elif hasattr(article, '__dict__'):
        data = article.__dict__
    else:
        # å…¶ä»–ç±»å‹ï¼Œç›´æ¥è¿”å›
        return article
    
    result = {}
    for key, value in data.items():
        if isinstance(value, datetime):
            # å°† datetime è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            result[key] = value.isoformat()
        elif isinstance(value, dict):
            # é€’å½’å¤„ç†åµŒå¥—å­—å…¸
            result[key] = serialize_article(value)
        elif hasattr(value, '__dict__'):
            # é€’å½’å¤„ç†åµŒå¥—å¯¹è±¡
            result[key] = serialize_article(value)
        elif isinstance(value, (list, tuple)):
            # å¤„ç†åˆ—è¡¨å’Œå…ƒç»„
            result[key] = [serialize_article(item) for item in value]
        else:
            # å…¶ä»–ç±»å‹ç›´æ¥èµ‹å€¼
            result[key] = value
    return result

# ä¸»ç¨‹åº
print("=" * 60)
print("å¼€å§‹æŠ“å–æ–°é—»...")
print("=" * 60)

all_articles = []
total_links_found = 0
total_articles_extracted = 0

# ç¬¬ä¸€æ­¥ï¼šä»æ¯ä¸ªé¦–é¡µæå–æ–°é—»é“¾æ¥
for homepage_url in homepage_urls:
    print(f"\nğŸ“° å¤„ç†é¦–é¡µ: {homepage_url}")
    try:
        # è·å–é¦–é¡µHTML
        response = requests.get(homepage_url, headers=headers, timeout=10)
        response.raise_for_status()
        
        # è§£æHTML
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æå–æ–°é—»é“¾æ¥
        article_links = extract_article_links(homepage_url, soup)
        total_links_found += len(article_links)
        
        print(f"  âœ“ æ‰¾åˆ° {len(article_links)} ä¸ªæ–°é—»é“¾æ¥")
        
        # ç¬¬äºŒæ­¥ï¼šå¯¹æ¯ä¸ªé“¾æ¥æå–æ–‡ç« å†…å®¹
        for i, link_info in enumerate(article_links, 1):
            article_url = link_info['url']
            homepage_title = link_info['title']
            
            print(f"  [{i}/{len(article_links)}] æå–: {homepage_title[:50]}...")
            
            try:
                # ä½¿ç”¨ newsplease æå–æ–‡ç« å†…å®¹
                article = NewsPlease.from_url(article_url, request_args=request_args)
                
                if article and article.title:
                    # åºåˆ—åŒ–æ–‡ç« æ•°æ®
                    article_data = serialize_article(article)
                    
                    # æ·»åŠ é¦–é¡µä¿¡æ¯
                    if article_data:
                        article_data['homepage_title'] = homepage_title
                        article_data['homepage_source'] = homepage_url
                        article_data['extracted_at'] = datetime.now().isoformat()
                    
                    all_articles.append(article_data)
                    total_articles_extracted += 1
                    print(f"    âœ“ æˆåŠŸæå–")
                else:
                    print(f"    âœ— æ— æ³•æå–å†…å®¹")
                
                # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(1)
                
            except Exception as e:
                print(f"    âœ— æå–å¤±è´¥: {str(e)[:50]}")
                continue
        
    except Exception as e:
        print(f"  âœ— å¤„ç†é¦–é¡µå¤±è´¥: {str(e)}")
        continue

# ç¬¬ä¸‰æ­¥ï¼šä¿å­˜åˆ°JSONæ–‡ä»¶
print(f"\n" + "=" * 60)
print(f"æŠ“å–å®Œæˆï¼")
print(f"  æ‰¾åˆ°é“¾æ¥: {total_links_found} ä¸ª")
print(f"  æˆåŠŸæå–: {total_articles_extracted} ç¯‡æ–‡ç« ")
print("=" * 60)


time_stamp = datetime.today().date()
output_dir = 'data'
os.makedirs(output_dir, exist_ok=True)
output_file = os.path.join(output_dir, f'news_{time_stamp}.json')
with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(all_articles, f, ensure_ascii=False, indent=2)

print(f"\nâœ… æ‰€æœ‰æ–‡ç« æ•°æ®å·²ä¿å­˜åˆ° {output_file}")
print(f"   å…± {len(all_articles)} ç¯‡æ–‡ç« ")
