#!/usr/bin/env python3
"""
Guardian.ng æ–°é—»æŠ“å–è¡¥ä¸ç¨‹åº
ç”±äºguardian.ngä½¿ç”¨äº†Cloudflareä¿æŠ¤ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
"""

from newsplease import NewsPlease
import json
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import urljoin, urlparse
import time
import os

# å°è¯•å¯¼å…¥cloudscraperï¼ˆä¸“é—¨ç”¨äºç»•è¿‡Cloudflareï¼‰
try:
    import cloudscraper
    CLOUDSCRAPER_AVAILABLE = True
except ImportError:
    CLOUDSCRAPER_AVAILABLE = False

# å°è¯•å¯¼å…¥seleniumï¼ˆç”¨äºæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ï¼‰
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, WebDriverException
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False

# Guardian.ng ç‰¹å®šé…ç½®
GUARDIAN_HOMEPAGE = 'https://guardian.ng/'

# å¢å¼ºçš„è¯·æ±‚å¤´ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
def get_headers():
    """è·å–è¯·æ±‚å¤´ï¼Œæ”¯æŒå¤šç§User-Agentè½®æ¢"""
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
    ]
    import random
    return {
        'User-Agent': random.choice(user_agents),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'Accept-Language': 'en-US,en;q=0.9',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
        'Cache-Control': 'max-age=0',
        'DNT': '1',
        'Referer': 'https://www.google.com/',
    }

# request_argsä¼šåœ¨è¿è¡Œæ—¶åŠ¨æ€ç”Ÿæˆ

def extract_guardian_article_links(homepage_url, soup):
    """
    ä¸“é—¨é’ˆå¯¹guardian.ngçš„é“¾æ¥æå–æ–¹æ³•
    å°è¯•å¤šç§é€‰æ‹©å™¨ç­–ç•¥
    """
    article_links = []
    base_domain = urlparse(homepage_url).netloc
    found_links = set()
    
    # Guardian.ng ç‰¹å®šçš„é€‰æ‹©å™¨
    guardian_selectors = [
        # å°è¯•å¸¸è§çš„WordPress/æ–°é—»ä¸»é¢˜é€‰æ‹©å™¨
        'article a',
        '.post a',
        '.entry-title a',
        '.post-title a',
        'h1 a', 'h2 a', 'h3 a', 'h4 a',
        '.title a',
        '.headline a',
        '.news-title a',
        '.article-title a',
        # å°è¯•ç‰¹å®šçš„ç±»å
        '.story-link',
        '.article-link',
        '.news-link',
        # å°è¯•åŒ…å«ç‰¹å®šè·¯å¾„çš„é“¾æ¥
        'a[href*="/news/"]',
        'a[href*="/article/"]',
        'a[href*="/story/"]',
        'a[href*="/202"]',  # åŒ…å«å¹´ä»½çš„é“¾æ¥ï¼ˆé€šå¸¸æ˜¯æ–‡ç« ï¼‰
        # å°è¯•åˆ—è¡¨é¡¹ä¸­çš„é“¾æ¥
        'li.article a',
        'li.post a',
        'li.news-item a',
        '.list-item a',
        '.item a',
    ]
    
    print(f"  ğŸ” å°è¯• {len(guardian_selectors)} ç§é€‰æ‹©å™¨ç­–ç•¥...")
    
    for selector in guardian_selectors:
        try:
            links = soup.select(selector)
            for link in links:
                href = link.get('href')
                if not href:
                    continue
                
                # è½¬æ¢ä¸ºç»å¯¹URL
                full_url = urljoin(homepage_url, href)
                parsed = urlparse(full_url)
                
                # è¿‡æ»¤æ¡ä»¶
                if (parsed.netloc == base_domain and 
                    full_url not in found_links and
                    full_url != homepage_url and
                    not any(x in full_url.lower() for x in [
                        '/category/', '/tag/', '/author/', '/page/', 
                        '/archive/', '/search/', '/contact/', '/about/',
                        '/privacy/', '/terms/', '/sitemap/', '/feed/',
                        '/amp/', '/#', 'javascript:', 'mailto:'
                    ]) and
                    len(full_url) > len(homepage_url) + 10):  # URLé•¿åº¦è¿‡æ»¤
                    
                    # å°è¯•è·å–æ ‡é¢˜
                    title = None
                    # å…ˆå°è¯•ä»é“¾æ¥æ–‡æœ¬è·å–
                    title = link.get_text(strip=True)
                    # å¦‚æœé“¾æ¥æ–‡æœ¬ä¸ºç©ºï¼Œå°è¯•ä»çˆ¶å…ƒç´ è·å–
                    if not title or len(title) < 10:
                        parent = link.parent
                        if parent:
                            title = parent.get_text(strip=True)
                    # å¦‚æœè¿˜æ˜¯ä¸ºç©ºï¼Œå°è¯•ä»dataå±æ€§è·å–
                    if not title or len(title) < 10:
                        title = link.get('title', '') or link.get('data-title', '')
                    
                    if title and len(title) > 10:
                        found_links.add(full_url)
                        article_links.append({
                            'title': title,
                            'url': full_url,
                            'source': homepage_url
                        })
        except Exception as e:
            continue
    
    # å¦‚æœä¸Šé¢çš„é€‰æ‹©å™¨éƒ½æ²¡æ‰¾åˆ°ï¼Œå°è¯•æ›´é€šç”¨çš„æ–¹æ³•
    if not article_links:
        print(f"  âš ï¸  æ ‡å‡†é€‰æ‹©å™¨æœªæ‰¾åˆ°é“¾æ¥ï¼Œå°è¯•é€šç”¨æ–¹æ³•...")
        all_links = soup.find_all('a', href=True)
        for link in all_links:
            href = link.get('href')
            if not href:
                continue
            
            full_url = urljoin(homepage_url, href)
            parsed = urlparse(full_url)
            
            # æ›´ä¸¥æ ¼çš„è¿‡æ»¤
            if (parsed.netloc == base_domain and 
                full_url not in found_links and
                full_url != homepage_url and
                len(full_url) > len(homepage_url) + 15 and  # æ›´é•¿çš„URLé€šå¸¸æ˜¯æ–‡ç« 
                not any(x in full_url.lower() for x in [
                    '/category/', '/tag/', '/author/', '/page/', '/archive/',
                    '/search/', '/contact/', '/about/', '/privacy/', '/terms/',
                    '/sitemap/', '/feed/', '/amp/', '/#', 'javascript:', 'mailto:',
                    '.jpg', '.jpeg', '.png', '.gif', '.pdf', '.zip'
                ]) and
                # æ£€æŸ¥URLè·¯å¾„æ˜¯å¦åŒ…å«æ—¥æœŸæˆ–æ–‡ç« æ ‡è¯†
                (any(x in full_url.lower() for x in ['/news/', '/article/', '/story/', '/202', '/2025', '/2026']) or
                 len(parsed.path.split('/')) >= 3)):  # è·¯å¾„è‡³å°‘3æ®µ
                
                title = link.get_text(strip=True)
                if not title or len(title) < 10:
                    # å°è¯•ä»çˆ¶å…ƒç´ è·å–
                    parent = link.parent
                    if parent:
                        title = parent.get_text(strip=True)
                
                if title and len(title) > 10:
                    found_links.add(full_url)
                    article_links.append({
                        'title': title,
                        'url': full_url,
                        'source': homepage_url
                    })
    
    # å»é‡å¹¶é™åˆ¶æ•°é‡
    seen = set()
    unique_links = []
    for item in article_links:
        if item['url'] not in seen:
            seen.add(item['url'])
            unique_links.append(item)
            if len(unique_links) >= 20:  # é™åˆ¶æœ€å¤š20æ¡
                break
    
    return unique_links

def serialize_article(article):
    """å°†æ–‡ç« å¯¹è±¡æˆ–å­—å…¸è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸"""
    if article is None:
        return None
    
    if isinstance(article, dict):
        data = article
    elif hasattr(article, '__dict__'):
        data = article.__dict__
    else:
        return article
    
    result = {}
    for key, value in data.items():
        if isinstance(value, datetime):
            result[key] = value.isoformat()
        elif isinstance(value, dict):
            result[key] = serialize_article(value)
        elif hasattr(value, '__dict__'):
            result[key] = serialize_article(value)
        elif isinstance(value, (list, tuple)):
            result[key] = [serialize_article(item) for item in value]
        else:
            result[key] = value
    return result

def try_selenium_method():
    """ä½¿ç”¨Seleniumæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨è·å–æ–‡ç« é“¾æ¥"""
    article_links = []
    driver = None
    
    try:
        # é…ç½®Chromeé€‰é¡¹
        chrome_options = Options()
        chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # å°è¯•åˆ›å»ºChromeé©±åŠ¨
        try:
            driver = webdriver.Chrome(options=chrome_options)
        except WebDriverException as e:
            if 'chromedriver' in str(e).lower() or 'path' in str(e).lower():
                print(f"    âš ï¸  Chromeé©±åŠ¨æœªæ‰¾åˆ°ï¼Œè¯·å®‰è£…: sudo apt-get install chromium-chromedriver")
                print(f"    æˆ–ä¸‹è½½: https://chromedriver.chromium.org/")
                return []
            raise
        
        # è®¿é—®é¦–é¡µ
        print(f"    ğŸ”„ è®¿é—®é¦–é¡µ: {GUARDIAN_HOMEPAGE}")
        driver.get(GUARDIAN_HOMEPAGE)
        
        # ç­‰å¾…é¡µé¢åŠ è½½ï¼ˆç­‰å¾…CloudflareéªŒè¯å®Œæˆï¼‰
        try:
            WebDriverWait(driver, 15).until(
                lambda d: 'Just a moment' not in d.page_source and 
                         'challenge-platform' not in d.page_source
            )
            print(f"    âœ“ é¡µé¢åŠ è½½å®Œæˆ")
        except TimeoutException:
            print(f"    âš ï¸  é¡µé¢åŠ è½½è¶…æ—¶ï¼ˆå¯èƒ½ä»åœ¨CloudflareéªŒè¯ä¸­ï¼‰")
            # ç»§ç»­å°è¯•è§£æ
        
        # è§£æé¡µé¢è·å–é“¾æ¥
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        links = extract_guardian_article_links(GUARDIAN_HOMEPAGE, soup)
        article_links.extend(links)
        
        # å¦‚æœé¦–é¡µæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•è®¿é—®åˆ†ç±»é¡µé¢
        if not article_links:
            news_sections = [
                'https://guardian.ng/news/',
                'https://guardian.ng/politics/',
                'https://guardian.ng/business/',
            ]
            
            for section_url in news_sections:
                try:
                    print(f"    ğŸ”„ è®¿é—®åˆ†ç±»é¡µé¢: {section_url}")
                    driver.get(section_url)
                    time.sleep(3)  # ç­‰å¾…é¡µé¢åŠ è½½
                    
                    soup = BeautifulSoup(driver.page_source, 'html.parser')
                    section_links = extract_guardian_article_links(section_url, soup)
                    if section_links:
                        article_links.extend(section_links)
                        print(f"    âœ“ ä» {section_url} æå–äº† {len(section_links)} ä¸ªé“¾æ¥")
                        if len(article_links) >= 20:
                            break
                except Exception as e:
                    continue
        
        return article_links
        
    except Exception as e:
        print(f"    âš ï¸  Seleniumæ–¹æ³•å¤±è´¥: {str(e)[:50]}")
        return []
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass

def crawl_guardian():
    """ä¸“é—¨æŠ“å–Guardian.ngçš„æ–°é—»"""
    print("=" * 60)
    print("Guardian.ng æ–°é—»æŠ“å–è¡¥ä¸ç¨‹åº")
    print("=" * 60)
    
    all_articles = []
    total_links_found = 0
    total_articles_extracted = 0
    article_links = []  # åˆå§‹åŒ–
    
    print(f"\nğŸ“° å¤„ç†é¦–é¡µ: {GUARDIAN_HOMEPAGE}")
    
    try:
        # ç­–ç•¥1: å°è¯•ä½¿ç”¨Seleniumï¼ˆæœ€å¯é ï¼Œä½†éœ€è¦æµè§ˆå™¨é©±åŠ¨ï¼‰
        if SELENIUM_AVAILABLE:
            print(f"  ğŸ”„ å°è¯•ä½¿ç”¨Seleniumï¼ˆæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ï¼‰...")
            article_links = try_selenium_method()
            if article_links:
                print(f"  âœ“ Seleniumæ–¹æ³•æˆåŠŸï¼Œæ‰¾åˆ° {len(article_links)} ä¸ªé“¾æ¥")
            else:
                print(f"  âš ï¸  Seleniumæ–¹æ³•å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•...")
        
        # ç­–ç•¥2: å¦‚æœSeleniumå¤±è´¥æˆ–ä¸å¯ç”¨ï¼Œå°è¯•cloudscraper
        if not article_links:
            print(f"  ğŸ”„ å°è¯•ä½¿ç”¨cloudscraper...")
            if CLOUDSCRAPER_AVAILABLE:
                print(f"  âœ“ ä½¿ç”¨ cloudscraper åº“ï¼ˆä¸“é—¨ç»•è¿‡Cloudflareï¼‰")
                session = cloudscraper.create_scraper()
                session.headers.update(get_headers())
            else:
                session = requests.Session()
                session.headers.update(get_headers())
        
        # æ–¹æ³•1: ç›´æ¥å°è¯•RSS feedï¼ˆä¸è®¿é—®é¦–é¡µï¼‰
        rss_urls = [
            'https://guardian.ng/feed/',
            'https://guardian.ng/rss/',
            'https://guardian.ng/news/feed/',
            'https://guardian.ng/feed/rss/',
            'https://guardian.ng/?feed=rss2',
        ]
        
        rss_success = False
        
        for rss_url in rss_urls:
            try:
                print(f"  ğŸ”„ å°è¯•RSS feed: {rss_url}")
                rss_response = session.get(rss_url, timeout=10, allow_redirects=True)
                print(f"    çŠ¶æ€ç : {rss_response.status_code}")
                if rss_response.status_code == 200:
                    content_type = rss_response.headers.get('content-type', '').lower()
                    content = rss_response.text
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„RSS/XMLå†…å®¹
                    if ('xml' in content_type or 'rss' in content_type or 'atom' in content_type or 
                        content.strip().startswith('<?xml') or '<rss' in content or '<feed' in content):
                        print(f"  âœ“ æ‰¾åˆ°RSS feed: {rss_url}")
                        # è§£æRSS
                        try:
                            from xml.etree import ElementTree as ET
                            root = ET.fromstring(rss_response.content)
                            
                            # æå–RSSä¸­çš„æ–‡ç« é“¾æ¥
                            for item in root.findall('.//item'):
                                title_elem = item.find('title')
                                link_elem = item.find('link')
                                if title_elem is not None and link_elem is not None:
                                    title = title_elem.text or ''
                                    url = link_elem.text or ''
                                    if title and url:
                                        article_links.append({
                                            'title': title.strip(),
                                            'url': url.strip(),
                                            'source': GUARDIAN_HOMEPAGE
                                        })
                            
                            if article_links:
                                rss_success = True
                                total_links_found = len(article_links)
                                print(f"  âœ“ ä»RSSæå–äº† {len(article_links)} ä¸ªé“¾æ¥")
                                break
                        except Exception as e:
                            print(f"    âš ï¸  RSSè§£æå¤±è´¥: {str(e)[:50]}")
                            continue
            except requests.exceptions.RequestException as e:
                print(f"    âš ï¸  RSSè¯·æ±‚å¤±è´¥: {str(e)[:50]}")
                continue
            except Exception as e:
                print(f"    âš ï¸  å…¶ä»–é”™è¯¯: {str(e)[:50]}")
                continue
            
            # å¦‚æœRSSæˆåŠŸï¼Œè·³è¿‡é¦–é¡µè®¿é—®
            if not rss_success:
                # æ–¹æ³•2: å°è¯•è®¿é—®é¦–é¡µ
                print(f"  ğŸ”„ RSSæ–¹æ³•å¤±è´¥ï¼Œå°è¯•è®¿é—®é¦–é¡µ...")
                response = session.get(GUARDIAN_HOMEPAGE, timeout=15, allow_redirects=True)
            
            # æ£€æŸ¥çŠ¶æ€ç 
            if response.status_code == 403:
                print(f"  âš ï¸  æ”¶åˆ°403é”™è¯¯ï¼Œå°è¯•å…¶ä»–æ–¹æ³•...")
                # å°è¯•è®¿é—®åˆ†ç±»é¡µé¢
                news_sections = [
                    'https://guardian.ng/news/',
                    'https://guardian.ng/politics/',
                    'https://guardian.ng/business/',
                ]
                
                for section_url in news_sections:
                    try:
                        print(f"  ğŸ”„ å°è¯•è®¿é—®åˆ†ç±»é¡µé¢: {section_url}")
                        section_response = session.get(section_url, timeout=10)
                        print(f"    çŠ¶æ€ç : {section_response.status_code}")
                        if section_response.status_code == 200:
                            soup = BeautifulSoup(section_response.content, 'html.parser')
                            section_links = extract_guardian_article_links(section_url, soup)
                            if section_links:
                                article_links.extend(section_links)
                                print(f"  âœ“ ä» {section_url} æå–äº† {len(section_links)} ä¸ªé“¾æ¥")
                                if len(article_links) >= 20:
                                    break
                    except Exception as e:
                        continue
            else:
                response.raise_for_status()
                
                # æ£€æŸ¥æ˜¯å¦è¢«Cloudflareæ‹¦æˆª
                if 'Just a moment' in response.text or 'challenge-platform' in response.text or 'cf-browser-verification' in response.text:
                    print(f"  âš ï¸  æ£€æµ‹åˆ°Cloudflareä¿æŠ¤")
                    # å¦‚æœé¦–é¡µè¢«æ‹¦æˆªï¼Œarticle_linksåº”è¯¥å·²ç»åœ¨ä¸Šé¢ä»åˆ†ç±»é¡µé¢è·å–äº†
                else:
                    # æ­£å¸¸è§£æHTML
                    soup = BeautifulSoup(response.content, 'html.parser')
                    homepage_links = extract_guardian_article_links(GUARDIAN_HOMEPAGE, soup)
                    if homepage_links:
                        article_links.extend(homepage_links)
                        print(f"  âœ“ ä»é¦–é¡µæå–äº† {len(homepage_links)} ä¸ªé“¾æ¥")
        
        # å»é‡
        if article_links:
            seen_urls = set()
            unique_links = []
            for link in article_links:
                if link['url'] not in seen_urls:
                    seen_urls.add(link['url'])
                    unique_links.append(link)
            article_links = unique_links
            total_links_found = len(article_links)
            print(f"  âœ“ å»é‡åå…± {total_links_found} ä¸ªæ–°é—»é“¾æ¥")
        
        if not article_links:
            print(f"  âŒ æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œæ— æ³•è·å–æ–‡ç« é“¾æ¥")
            print(f"  ğŸ’¡ å»ºè®®ï¼š")
            print(f"     1. æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œé˜²ç«å¢™è®¾ç½®")
            print(f"     2. ä½¿ç”¨Seleniumç­‰å·¥å…·å¤„ç†JavaScriptï¼ˆéœ€è¦å®‰è£…seleniumï¼‰")
            print(f"     3. ä½¿ç”¨ä»£ç†æœåŠ¡")
            print(f"     4. æ‰‹åŠ¨è®¿é—®guardian.ngè·å–æ–‡ç« é“¾æ¥")
            return []
        
        # æå–æ–‡ç« å†…å®¹
        if not article_links:
            print(f"  âš ï¸  æœªæ‰¾åˆ°ä»»ä½•æ–°é—»é“¾æ¥")
            return []
        
        for i, link_info in enumerate(article_links, 1):
            article_url = link_info['url']
            homepage_title = link_info['title']
            
            print(f"  [{i}/{len(article_links)}] æå–: {homepage_title[:50]}...")
            
            try:
                # ä½¿ç”¨ newsplease æå–æ–‡ç« å†…å®¹ï¼ˆåŠ¨æ€ç”Ÿæˆè¯·æ±‚å¤´ï¼‰
                current_request_args = {'headers': get_headers()}
                article = NewsPlease.from_url(article_url, request_args=current_request_args)
                
                if article and article.title:
                    article_data = serialize_article(article)
                    
                    if article_data:
                        article_data['homepage_title'] = homepage_title
                        article_data['homepage_source'] = GUARDIAN_HOMEPAGE
                        article_data['extracted_at'] = datetime.now().isoformat()
                    
                    all_articles.append(article_data)
                    total_articles_extracted += 1
                    print(f"    âœ“ æˆåŠŸæå–")
                else:
                    print(f"    âœ— æ— æ³•æå–å†…å®¹")
                
                time.sleep(1)  # å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                
            except Exception as e:
                print(f"    âœ— æå–å¤±è´¥: {str(e)[:50]}")
                continue
        
    except requests.exceptions.RequestException as e:
        print(f"  âœ— ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
        return []
    except Exception as e:
        print(f"  âœ— å¤„ç†å¤±è´¥: {str(e)}")
        return []
    
    print(f"\n" + "=" * 60)
    print(f"Guardian.ng æŠ“å–å®Œæˆï¼")
    print(f"  æ‰¾åˆ°é“¾æ¥: {total_links_found} ä¸ª")
    print(f"  æˆåŠŸæå–: {total_articles_extracted} ç¯‡æ–‡ç« ")
    print("=" * 60)
    
    return all_articles

def main():
    """ä¸»å‡½æ•°"""
    articles = crawl_guardian()
    
    if articles:
        # ä¿å­˜åˆ°æ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = 'data'
        os.makedirs(output_dir, exist_ok=True)
        output_file = os.path.join(output_dir, f'guardian_news_{timestamp}.json')
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(articles, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… Guardian.ngæ–‡ç« æ•°æ®å·²ä¿å­˜åˆ° {output_file}")
        print(f"   å…± {len(articles)} ç¯‡æ–‡ç« ")
    else:
        print(f"\nâš ï¸  æœªæŠ“å–åˆ°ä»»ä½•æ–‡ç« ")

if __name__ == '__main__':
    main()
