import json
import os
from datetime import datetime
from typing import List, Dict, Optional
import time
import hashlib

# Import configuration and AI client
from config import config, get_config, is_available, set_provider
from ai_client import AIClient

# Configuration
PROMPT_TEMPLATE_FILE = os.getenv('PROMPT_TEMPLATE_FILE', 'prompt_template.txt')
DATA_DIR = 'data'
REPORT_DIR = 'report'

# Get AI provider from environment variable or config file
AI_PROVIDER = os.getenv('AI_PROVIDER', 'gemini').lower()

def load_prompt_template() -> str:
    """Load prompt template file
    Priority: environment variable PROMPT_TEMPLATE > file prompt_template.txt > default template
    """
    # Priority: read from environment variable (for GitHub Actions, etc.)
    prompt_from_env = os.getenv('PROMPT_TEMPLATE')
    if prompt_from_env:
        print("âœ“ ä»ç¯å¢ƒå˜é‡åŠ è½½ prompt")
        return prompt_from_env.strip()
    
    # If environment variable doesn't exist, try reading from file
    try:
        if os.path.exists(PROMPT_TEMPLATE_FILE):
            with open(PROMPT_TEMPLATE_FILE, 'r', encoding='utf-8') as f:
                print("âœ“ ä»æ–‡ä»¶åŠ è½½ prompt")
                return f.read().strip()
        else:
            # If file doesn't exist, return default template
            print(f"âš ï¸  Promptæ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {PROMPT_TEMPLATE_FILE}ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿")
            return """è¯·ä½ å°†ä»¥ä¸‹æ–°é—»ç¿»è¯‘ä¸ºä¸­æ–‡ã€‚"""
    except Exception as e:
        print(f"âš ï¸  åŠ è½½promptæ¨¡æ¿å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿")
        return """è¯·ä½ å°†ä»¥ä¸‹æ–°é—»ç¿»è¯‘ä¸ºä¸­æ–‡ã€‚"""

def process_article_with_ai(ai_client: AIClient, article: Dict) -> Optional[Dict]:
    """
    Process a single article with AI:
    1. Filter low-quality or irrelevant articles
    2. Extract key information
    3. Translate to Chinese
    """
    if not ai_client:
        return None
    
    # Prepare article content
    title = article.get('title', '')
    description = article.get('description', '')
    maintext = article.get('maintext', '')
    authors = article.get('authors', [])
    date_publish = article.get('date_publish', '')
    source = article.get('source_domain', '')
    
    # If main text is too short, might not be a complete article
    if not maintext or len(maintext) < 100:
        return None
    
    # Limit main text length (to avoid token limits)
    maintext_preview = maintext[:3000] if len(maintext) > 3000 else maintext
    if len(maintext) > 3000:
        maintext_preview += "\n\n[æ–‡ç« å†…å®¹è¾ƒé•¿ï¼Œå·²æˆªæ–­]"
    
    # Load and format prompt template
    prompt_template = load_prompt_template()
    prompt = prompt_template.format(
        title=title,
        description=description,
        authors=', '.join(authors) if authors else 'æœªçŸ¥',
        date_publish=date_publish,
        source=source,
        maintext_preview=maintext_preview
    )

    try:
        # Use unified AI client interface
        response = ai_client.generate_content(prompt)
        
        # Check for errors
        if 'error' in response:
            error = response['error']
            if 'å®‰å…¨è¿‡æ»¤å™¨' in error or 'SAFETY' in str(response.get('finish_reason', '')):
                print(f"  âš ï¸  å†…å®¹è¢«å®‰å…¨è¿‡æ»¤å™¨é˜»æ­¢")
                # Return basic data
                return {
                    "original": {
                        "title": title,
                        "description": description,
                        "maintext": maintext,
                        "authors": authors,
                        "date_publish": date_publish,
                        "source_domain": source,
                        "url": article.get('url', ''),
                        "homepage_source": article.get('homepage_source', ''),
                    },
                    "processed": {
                        "is_valid": True,
                        "category": "å…¶ä»–",
                        "key_points": [],
                        "title_zh": "",
                        "description_zh": "",
                        "summary_zh": "å†…å®¹è¢«å®‰å…¨è¿‡æ»¤å™¨é˜»æ­¢ï¼Œæ— æ³•è¿›è¡ŒAIå¤„ç†",
                        "maintext_zh": "",
                    },
                    "metadata": {
                        "processed_at": datetime.now().isoformat(),
                        "source": f"blocked_by_safety_filter_{ai_client.provider}"
                    }
                }
            else:
                print(f"  âš ï¸  AIå¤„ç†å¤±è´¥: {error}")
                return None
        
        result_text = response.get('text', '')
        if not result_text:
            print(f"  âš ï¸  å“åº”ä¸­æ²¡æœ‰æ–‡æœ¬å†…å®¹")
            return None
        
        # Try to extract JSON (might be returned in markdown code block format)
        if '```json' in result_text:
            result_text = result_text.split('```json')[1].split('```')[0].strip()
        elif '```' in result_text:
            parts = result_text.split('```')
            for i, part in enumerate(parts):
                if i % 2 == 1:  # Code block content
                    try:
                        json.loads(part.strip())
                        result_text = part.strip()
                        break
                    except:
                        continue
        
        # If still contains JSON object, try to extract
        if '{' in result_text and '}' in result_text:
            start = result_text.find('{')
            end = result_text.rfind('}') + 1
            result_text = result_text[start:end]
        
        # Parse JSON
        ai_result = json.loads(result_text)
        
        # Merge original data and AI processing results
        is_valid = ai_result.get('is_valid', False)
        
        # If article is invalid (filtered), ensure all fields are empty
        if not is_valid:
            processed_article = {
                "original": {
                    "title": title,
                    "description": description,
                    "maintext": maintext,
                    "authors": authors,
                    "date_publish": date_publish,
                    "source_domain": source,
                    "url": article.get('url', ''),
                    "homepage_source": article.get('homepage_source', ''),
                },
                "processed": {
                    "is_valid": False,
                    "category": "",  # Invalid article, category is empty
                    "key_points": [],  # Invalid article, key points are empty
                    "title_zh": "",  # Invalid article, no translation
                    "description_zh": "",  # Invalid article, no translation
                    "summary_zh": "",  # Invalid article, no translation
                    "maintext_zh": "",  # Invalid article, no translation
                },
                "metadata": {
                    "processed_at": datetime.now().isoformat(),
                    "source": f"{ai_client.provider}-{ai_client.model_name}",
                    "filtered_reason": "éä¸¥è‚ƒæ–°é—»ï¼ˆèŠ±è¾¹/å¨±ä¹/ä½“è‚²/å…»ç”Ÿä¿å¥ç­‰ï¼‰"
                }
            }
        else:
            # Valid article, process normally
            processed_article = {
                "original": {
                    "title": title,
                    "description": description,
                    "maintext": maintext,
                    "authors": authors,
                    "date_publish": date_publish,
                    "source_domain": source,
                    "url": article.get('url', ''),
                    "homepage_source": article.get('homepage_source', ''),
                },
                "processed": {
                    "is_valid": True,
                    "category": ai_result.get('category', 'å…¶ä»–æ— å…³æ–°é—»'),
                    "key_points": ai_result.get('key_points', []),
                    "title_zh": ai_result.get('title_zh', ''),
                    "description_zh": ai_result.get('description_zh', ''),
                    "summary_zh": ai_result.get('summary_zh', ''),
                    "maintext_zh": ai_result.get('maintext_zh', ''),
                },
                "metadata": {
                    "processed_at": datetime.now().isoformat(),
                    "source": f"{ai_client.provider}-{ai_client.model_name}"
                }
            }
        
        return processed_article
        
    except json.JSONDecodeError as e:
        print(f"  âš ï¸  JSONè§£æå¤±è´¥: {e}")
        if 'result_text' in locals():
            print(f"  å“åº”å†…å®¹: {result_text[:200]}")
        return None
    except Exception as e:
        print(f"  âš ï¸  AIå¤„ç†å¤±è´¥: {e}")
        return None

def find_latest_articles_file() -> Optional[str]:
    """Find the latest article JSON file from the data folder"""
    if not os.path.exists(DATA_DIR):
        print(f"âŒ æ•°æ®æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {DATA_DIR}")
        return None
    
    # Find all JSON files
    json_files = []
    for filename in os.listdir(DATA_DIR):
        if filename.endswith('.json'):
            filepath = os.path.join(DATA_DIR, filename)
            if os.path.isfile(filepath):
                mtime = os.path.getmtime(filepath)
                json_files.append((mtime, filepath, filename))
    
    if not json_files:
        print(f"âŒ åœ¨ {DATA_DIR} æ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°JSONæ–‡ä»¶")
        return None
    
    # Sort by modification time, return the latest
    json_files.sort(reverse=True)
    latest_file = json_files[0][1]
    print(f"âœ“ æ‰¾åˆ°æœ€æ–°æ–‡ç« æ–‡ä»¶: {json_files[0][2]}")
    return latest_file

def get_article_id(article: Dict) -> str:
    """Generate unique article ID (based on URL)"""
    url = article.get('url', '')
    if url:
        return hashlib.md5(url.encode()).hexdigest()
    # If no URL, use combination of title and source
    title = article.get('title', '')
    source = article.get('source_domain', '')
    return hashlib.md5(f"{title}_{source}".encode()).hexdigest()

def load_processed_cache(cache_file: str) -> Dict[str, Dict]:
    """Load processed article cache"""
    if os.path.exists(cache_file):
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
                print(f"  âœ“ åŠ è½½ç¼“å­˜: {len(cache_data)} ç¯‡å·²å¤„ç†æ–‡ç« ")
                return cache_data
        except Exception as e:
            print(f"  âš ï¸  åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            return {}
    return {}

def save_processed_cache(cache_file: str, processed_dict: Dict[str, Dict]):
    """Save processed article cache"""
    try:
        # Use temporary file to ensure atomic write
        temp_file = cache_file + '.tmp'
        with open(temp_file, 'w', encoding='utf-8') as f:
            json.dump(processed_dict, f, ensure_ascii=False, indent=2)
        os.replace(temp_file, cache_file)
    except Exception as e:
        print(f"  âš ï¸  ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

def save_intermediate_results(processed_articles: List[Dict], report_dir: str):
    """Save intermediate results"""
    try:
        intermediate_file = os.path.join(report_dir, 'report_intermediate.json')
        report = generate_report(processed_articles)
        with open(intermediate_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"  âš ï¸  ä¿å­˜ä¸­é—´ç»“æœå¤±è´¥: {e}")

def load_articles() -> List[Dict]:
    """Load article data (load the latest file from the data folder)"""
    # Priority: use file specified by environment variable
    articles_file = os.getenv('ARTICLES_FILE')
    
    if articles_file:
        # If file is specified, use it directly
        if not os.path.isabs(articles_file):
            articles_file = os.path.join(DATA_DIR, articles_file) if not os.path.exists(articles_file) else articles_file
    else:
        # Otherwise, find the latest file
        articles_file = find_latest_articles_file()
    
    if not articles_file:
        return []
    
    try:
        with open(articles_file, 'r', encoding='utf-8') as f:
            articles = json.load(f)
        print(f"âœ“ æˆåŠŸåŠ è½½ {len(articles)} ç¯‡æ–‡ç« ")
        return articles
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {articles_file}")
        return []
    except json.JSONDecodeError as e:
        print(f"âŒ JSONè§£æé”™è¯¯: {e}")
        return []

def generate_report(processed_articles: List[Dict]) -> Dict:
    """Generate summary report"""
    total = len(processed_articles)
    valid_articles = [a for a in processed_articles if a['processed']['is_valid']]
    invalid_count = total - len(valid_articles)
    
    # Statistics by category
    category_stats = {}
    for article in valid_articles:
        category = article['processed']['category']
        category_stats[category] = category_stats.get(category, 0) + 1
    
    # Statistics by source
    source_stats = {}
    for article in valid_articles:
        source = article['original']['source_domain']
        source_stats[source] = source_stats.get(source, 0) + 1
    
    report = {
        "summary": {
            "total_articles": total,
            "valid_articles": len(valid_articles),
            "invalid_articles": invalid_count,
            "processing_date": datetime.now().isoformat(),
        },
        "statistics": {
            "by_category": category_stats,
            "by_source": source_stats,
        },
        "articles": valid_articles
    }
    
    return report

def generate_markdown_report(report: Dict) -> str:
    """Generate Markdown format report"""
    md = []
    
    # Title
    md.append("# å°¼æ—¥åˆ©äºšæ–°é—»æ±‡æ€»æŠ¥å‘Š\n")
    md.append(f"**ç”Ÿæˆæ—¶é—´**: {report['summary']['processing_date']}\n")
    
    # Summary
    md.append("## ğŸ“Š æ•°æ®æ‘˜è¦\n")
    md.append(f"- **æ€»æ–‡ç« æ•°**: {report['summary']['total_articles']}")
    md.append(f"- **æœ‰æ•ˆæ–‡ç« **: {report['summary']['valid_articles']}")
    md.append(f"- **æ— æ•ˆæ–‡ç« **: {report['summary']['invalid_articles']}\n")
    
    # Category statistics
    md.append("## ğŸ“ åˆ†ç±»ç»Ÿè®¡\n")
    for category, count in sorted(report['statistics']['by_category'].items(), 
                                  key=lambda x: x[1], reverse=True):
        md.append(f"- **{category}**: {count} ç¯‡")
    md.append("")
    
    # Source statistics
    md.append("## ğŸ“° æ¥æºç»Ÿè®¡\n")
    for source, count in sorted(report['statistics']['by_source'].items(), 
                               key=lambda x: x[1], reverse=True):
        md.append(f"- **{source}**: {count} ç¯‡")
    md.append("")
    
    # Article list
    md.append("## ğŸ“„ æ–‡ç« è¯¦æƒ…\n")
    md.append("---\n")
    
    for i, article in enumerate(report['articles'], 1):
        original = article['original']
        processed = article['processed']
        
        md.append(f"### {i}. {processed['title_zh'] or original['title']}\n")
        md.append(f"**åŸæ–‡æ ‡é¢˜**: {original['title']}\n")
        md.append(f"**åˆ†ç±»**: {processed['category']}\n")
        md.append(f"**æ¥æº**: {original['source_domain']}\n")
        md.append(f"**ä½œè€…**: {', '.join(original['authors']) if original['authors'] else 'æœªçŸ¥'}\n")
        md.append(f"**å‘å¸ƒæ—¥æœŸ**: {original['date_publish']}\n")
        md.append(f"**é“¾æ¥**: {original['url']}\n")
        
        if processed['description_zh']:
            md.append(f"\n**æè¿°**:\n{processed['description_zh']}\n")
        
        if processed['key_points']:
            md.append("\n**å…³é”®è¦ç‚¹**:\n")
            for point in processed['key_points']:
                md.append(f"- {point}")
            md.append("")
        
        if processed['summary_zh']:
            md.append(f"\n**æ‘˜è¦**:\n{processed['summary_zh']}\n")
        
        if processed['maintext_zh']:
            md.append("\n**æ­£æ–‡ï¼ˆä¸­æ–‡ï¼‰**:\n")
            md.append(f"{processed['maintext_zh']}\n")
        
        md.append("\n---\n")
    
    return "\n".join(md)

def generate_html_report(report: Dict) -> str:
    """Generate HTML format report"""
    html = []
    
    # HTML header
    processing_date = report['summary']['processing_date']
    html.append(f"""<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>å°¼æ—¥åˆ©äºšæ–°é—»æ±‡æ€»æŠ¥å‘Š</title>
    <style>
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }}
        .container {{
            background: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        h1 {{
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }}
        h2 {{
            color: #34495e;
            margin-top: 30px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }}
        h3 {{
            color: #555;
            margin-top: 25px;
        }}
        .summary {{
            background: #ecf0f1;
            padding: 20px;
            border-radius: 5px;
            margin: 20px 0;
        }}
        .summary-item {{
            margin: 10px 0;
            font-size: 16px;
        }}
        .summary-item strong {{
            color: #2980b9;
        }}
        .stats {{
            display: flex;
            flex-wrap: wrap;
            gap: 15px;
            margin: 20px 0;
        }}
        .stat-card {{
            background: #fff;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 15px;
            min-width: 200px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        }}
        .stat-card h4 {{
            margin: 0 0 10px 0;
            color: #7f8c8d;
            font-size: 14px;
        }}
        .stat-card .value {{
            font-size: 24px;
            font-weight: bold;
            color: #2c3e50;
        }}
        .article {{
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 20px;
            margin: 20px 0;
            background: #fafafa;
        }}
        .article-header {{
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 15px;
        }}
        .article-title {{
            font-size: 20px;
            color: #2c3e50;
            margin: 0;
        }}
        .article-meta {{
            color: #7f8c8d;
            font-size: 14px;
            margin: 10px 0;
        }}
        .article-meta span {{
            margin-right: 15px;
        }}
        .key-points {{
            background: #e8f5e9;
            padding: 15px;
            border-radius: 5px;
            margin: 15px 0;
        }}
        .key-points ul {{
            margin: 0;
            padding-left: 20px;
        }}
        .key-points li {{
            margin: 8px 0;
        }}
        .content {{
            margin: 15px 0;
            line-height: 1.8;
        }}
        .link {{
            color: #3498db;
            text-decoration: none;
        }}
        .link:hover {{
            text-decoration: underline;
        }}
        .category-badge {{
            display: inline-block;
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 12px;
            font-weight: bold;
            background: #3498db;
            color: white;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ“° å°¼æ—¥åˆ©äºšæ–°é—»æ±‡æ€»æŠ¥å‘Š</h1>
        <div class="summary">
            <div class="summary-item"><strong>ç”Ÿæˆæ—¶é—´</strong>: {processing_date}</div>
        </div>
    """)
    
    # Data summary
    total_articles = report['summary']['total_articles']
    valid_articles = report['summary']['valid_articles']
    invalid_articles = report['summary']['invalid_articles']
    html.append(f"""
        <h2>ğŸ“Š æ•°æ®æ‘˜è¦</h2>
        <div class="stats">
            <div class="stat-card">
                <h4>æ€»æ–‡ç« æ•°</h4>
                <div class="value">{total_articles}</div>
            </div>
            <div class="stat-card">
                <h4>æœ‰æ•ˆæ–‡ç« </h4>
                <div class="value">{valid_articles}</div>
            </div>
            <div class="stat-card">
                <h4>æ— æ•ˆæ–‡ç« </h4>
                <div class="value">{invalid_articles}</div>
            </div>
        </div>
    """)
    
    # Category statistics
    html.append("<h2>ğŸ“ åˆ†ç±»ç»Ÿè®¡</h2>")
    html.append('<div class="stats">')
    for category, count in sorted(report['statistics']['by_category'].items(), 
                                  key=lambda x: x[1], reverse=True):
        html.append(f"""
            <div class="stat-card">
                <h4>{category}</h4>
                <div class="value">{count} ç¯‡</div>
            </div>
        """)
    html.append("</div>")
    
    # Source statistics
    html.append("<h2>ğŸ“° æ¥æºç»Ÿè®¡</h2>")
    html.append('<div class="stats">')
    for source, count in sorted(report['statistics']['by_source'].items(), 
                               key=lambda x: x[1], reverse=True):
        html.append(f"""
            <div class="stat-card">
                <h4>{source}</h4>
                <div class="value">{count} ç¯‡</div>
            </div>
        """)
    html.append("</div>")
    
    # Article list
    html.append("<h2>ğŸ“„ æ–‡ç« è¯¦æƒ…</h2>")
    
    for i, article in enumerate(report['articles'], 1):
        original = article['original']
        processed = article['processed']
        
        html.append('<div class="article">')
        html.append('<div class="article-header">')
        html.append(f'<h3 class="article-title">{i}. {processed["title_zh"] or original["title"]}</h3>')
        html.append(f'<span class="category-badge">{processed["category"]}</span>')
        html.append('</div>')
        
        html.append('<div class="article-meta">')
        html.append(f'<span><strong>åŸæ–‡æ ‡é¢˜</strong>: {original["title"]}</span>')
        html.append(f'<span><strong>æ¥æº</strong>: {original["source_domain"]}</span>')
        if original['authors']:
            html.append(f'<span><strong>ä½œè€…</strong>: {", ".join(original["authors"])}</span>')
        if original['date_publish']:
            html.append(f'<span><strong>å‘å¸ƒæ—¥æœŸ</strong>: {original["date_publish"]}</span>')
        html.append('</div>')
        
        if original['url']:
            html.append(f'<p><a href="{original["url"]}" class="link" target="_blank">æŸ¥çœ‹åŸæ–‡</a></p>')
        
        if processed['description_zh']:
            html.append(f'<div class="content"><strong>æè¿°</strong>:<br>{processed["description_zh"]}</div>')
        
        if processed['key_points']:
            html.append('<div class="key-points"><strong>å…³é”®è¦ç‚¹</strong>:<ul>')
            for point in processed['key_points']:
                html.append(f'<li>{point}</li>')
            html.append('</ul></div>')
        
        if processed['summary_zh']:
            html.append(f'<div class="content"><strong>æ‘˜è¦</strong>:<br>{processed["summary_zh"]}</div>')
        
        if processed['maintext_zh']:
            html.append(f'<div class="content"><strong>æ­£æ–‡ï¼ˆä¸­æ–‡ï¼‰</strong>:<br>{processed["maintext_zh"]}</div>')
        
        html.append('</div>')
    
    # HTML footer
    html.append("""
    </div>
</body>
</html>
    """)
    
    return "\n".join(html)

def main():
    """Main function"""
    print("=" * 60)
    print("AIæ–°é—»å¤„ç†ä¸æŠ¥å‘Šç”Ÿæˆ")
    print("=" * 60)
    
    # Display configuration status
    config.print_status()
    
    # Get AI provider (use local variable to avoid scope issues)
    current_provider = AI_PROVIDER
    
    # Initialize AI client
    if not is_available(current_provider):
        print(f"\nâš ï¸  {current_provider} ä¸å¯ç”¨ï¼Œå°è¯•æŸ¥æ‰¾å¯ç”¨çš„æä¾›å•†...")
        available = config.get_available_providers()
        if available:
            current_provider = available[0]
            print(f"âœ“ ä½¿ç”¨ {current_provider} ä½œä¸ºAIæä¾›å•†")
        else:
            print("\nâŒ æ²¡æœ‰å¯ç”¨çš„AIæä¾›å•†")
            print("   è¯·è®¾ç½®ç›¸åº”çš„APIå¯†é’¥ç¯å¢ƒå˜é‡ï¼š")
            print("   - GEMINI_API_KEY (for gemini)")
            print("   - DASHSCOPE_API_KEY (for tongyi)")
            print("   - DEEPSEEK_API_KEY (for deepseek)")
            print("   - OPENAI_API_KEY (for openai)")
            print("   - ANTHROPIC_API_KEY (for claude)")
            print("   - TENCENT_SECRET_ID & TENCENT_SECRET_KEY (for hunyuan)")
            return
    
    try:
        ai_client = AIClient(current_provider)
        print(f"âœ“ AIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ: {ai_client.provider} ({ai_client.model_name})")
    except Exception as e:
        print(f"\nâŒ AIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        print("   å°†ä½¿ç”¨åŸºç¡€å¤„ç†æ¨¡å¼ï¼ˆæ— AIåŠŸèƒ½ï¼‰")
        ai_client = None
    
    # Load articles
    print(f"\nğŸ“‚ åŠ è½½æ–‡ç« æ•°æ®...")
    articles = load_articles()
    
    if not articles:
        print("âŒ æ²¡æœ‰å¯å¤„ç†çš„æ–‡ç« ")
        return
    
    # Create report directory and cache file path
    today = datetime.now().strftime("%Y%m%d")
    report_date_dir = os.path.join(REPORT_DIR, today)
    os.makedirs(report_date_dir, exist_ok=True)
    cache_file = os.path.join(report_date_dir, 'processed_cache.json')
    
    # Load processed article cache (resume from breakpoint)
    print(f"\nğŸ“‹ æ£€æŸ¥å·²å¤„ç†ç¼“å­˜...")
    processed_cache = load_processed_cache(cache_file)
    if processed_cache:
        print(f"  âœ“ å‘ç° {len(processed_cache)} ç¯‡å·²å¤„ç†æ–‡ç« ï¼Œå°†è·³è¿‡è¿™äº›æ–‡ç« ")
    
    # Process articles
    print(f"\nğŸ¤– ä½¿ç”¨AIå¤„ç†æ–‡ç« ...")
    processed_articles = []
    save_interval = 5  # Save intermediate results every 5 articles
    skipped_count = 0
    
    try:
        for i, article in enumerate(articles, 1):
            article_id = get_article_id(article)
            
            # Check if already processed (resume from breakpoint)
            if article_id in processed_cache:
                print(f"\n[{i}/{len(articles)}] â­ï¸  è·³è¿‡ï¼ˆå·²å¤„ç†ï¼‰: {article.get('title', 'æ— æ ‡é¢˜')[:50]}...")
                processed_articles.append(processed_cache[article_id])
                skipped_count += 1
                continue
            
            print(f"\n[{i}/{len(articles)}] å¤„ç†: {article.get('title', 'æ— æ ‡é¢˜')[:50]}...")
            
            if ai_client:
                processed = process_article_with_ai(ai_client, article)
                if processed:
                    processed_articles.append(processed)
                    # Immediately save to cache (resume from breakpoint)
                    processed_cache[article_id] = processed
                    save_processed_cache(cache_file, processed_cache)
                    
                    # Save intermediate results every N articles
                    new_processed_count = len(processed_articles) - skipped_count
                    if new_processed_count > 0 and new_processed_count % save_interval == 0:
                        print(f"  ğŸ’¾ ä¿å­˜ä¸­é—´ç»“æœï¼ˆå·²å¤„ç† {len(processed_articles)} ç¯‡ï¼Œå…¶ä¸­æ–°å¤„ç† {new_processed_count} ç¯‡ï¼‰...")
                        save_intermediate_results(processed_articles, report_date_dir)
                    
                    if processed['processed']['is_valid']:
                        print(f"  âœ“ æœ‰æ•ˆæ–‡ç«  - åˆ†ç±»: {processed['processed']['category']}")
                    else:
                        print(f"  âœ— æ— æ•ˆæ–‡ç« ï¼ˆå·²è¿‡æ»¤ï¼‰")
                else:
                    print(f"  âš ï¸  å¤„ç†å¤±è´¥")
                    # Check if it's an insufficient balance error, if so, prompt user
                    if i == 1:  # Only prompt on first article failure
                        print(f"\nğŸ’¡ æç¤º: å¦‚æœçœ‹åˆ°'ä½™é¢ä¸è¶³'é”™è¯¯ï¼Œå¯ä»¥ï¼š")
                        print(f"   1. ä¸ºå½“å‰AIæä¾›å•†å……å€¼")
                        print(f"   2. åˆ‡æ¢åˆ°å…¶ä»–å¯ç”¨æä¾›å•†: export AI_PROVIDER='gemini' æˆ– 'tongyi'")
                        print(f"   3. æŸ¥çœ‹å¯ç”¨æä¾›å•†: python -c 'from config import config; config.print_status()'")
                
                # Add delay to avoid API rate limiting
                time.sleep(1)
            else:
                # If no AI model, use basic processing
                processed = {
                    "original": {
                        "title": article.get('title', ''),
                        "description": article.get('description', ''),
                        "maintext": article.get('maintext', ''),
                        "authors": article.get('authors', []),
                        "date_publish": article.get('date_publish', ''),
                        "source_domain": article.get('source_domain', ''),
                        "url": article.get('url', ''),
                        "homepage_source": article.get('homepage_source', ''),
                    },
                    "processed": {
                        "is_valid": bool(article.get('maintext')),
                        "category": "å…¶ä»–",
                        "key_points": [],
                        "title_zh": "",
                        "description_zh": "",
                        "summary_zh": "",
                        "maintext_zh": "",
                    },
                    "metadata": {
                        "processed_at": datetime.now().isoformat(),
                        "source": "basic"
                    }
                }
                processed_articles.append(processed)
                processed_cache[article_id] = processed
                save_processed_cache(cache_file, processed_cache)
    
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜å·²å¤„ç†çš„ç»“æœ...")
    except Exception as e:
        print(f"\n\nâŒ å¤„ç†è¿‡ç¨‹å‡ºé”™: {e}")
        print("ğŸ’¾ å°è¯•ä¿å­˜å·²å¤„ç†çš„ç»“æœ...")
    finally:
        # Save processed results regardless of exceptions
        if processed_articles:
            print(f"\nğŸ’¾ ä¿å­˜æœ€ç»ˆç»“æœï¼ˆå…± {len(processed_articles)} ç¯‡ï¼Œå…¶ä¸­è·³è¿‡ {skipped_count} ç¯‡ï¼‰...")
            save_intermediate_results(processed_articles, report_date_dir)
    
    # Generate final report (using processed results)
    if not processed_articles:
        print("\nâš ï¸  æ²¡æœ‰å·²å¤„ç†çš„æ–‡ç« ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
        return
    
    print(f"\nğŸ“Š ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")
    report = generate_report(processed_articles)
    
    # Save JSON report
    report_json_file = os.path.join(report_date_dir, 'report.json')
    with open(report_json_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    print(f"âœ“ JSONæŠ¥å‘Šå·²ä¿å­˜: {report_json_file}")
    
    # Generate and save Markdown report
    md_report = generate_markdown_report(report)
    report_md_file = os.path.join(report_date_dir, 'report.md')
    with open(report_md_file, 'w', encoding='utf-8') as f:
        f.write(md_report)
    print(f"âœ“ MarkdownæŠ¥å‘Šå·²ä¿å­˜: {report_md_file}")
    
    # Generate and save HTML report
    html_report = generate_html_report(report)
    report_html_file = os.path.join(report_date_dir, 'report.html')
    with open(report_html_file, 'w', encoding='utf-8') as f:
        f.write(html_report)
    print(f"âœ“ HTMLæŠ¥å‘Šå·²ä¿å­˜: {report_html_file}")
    
    # Clean up intermediate result files (keep final reports)
    intermediate_file = os.path.join(report_date_dir, 'report_intermediate.json')
    if os.path.exists(intermediate_file):
        try:
            os.remove(intermediate_file)
            print(f"âœ“ å·²æ¸…ç†ä¸­é—´ç»“æœæ–‡ä»¶")
        except:
            pass
    
    # Print summary
    print(f"\n" + "=" * 60)
    print("å¤„ç†å®Œæˆï¼")
    print("=" * 60)
    print(f"æ€»æ–‡ç« æ•°: {report['summary']['total_articles']}")
    print(f"æœ‰æ•ˆæ–‡ç« : {report['summary']['valid_articles']}")
    print(f"æ— æ•ˆæ–‡ç« : {report['summary']['invalid_articles']}")
    print(f"\nåˆ†ç±»ç»Ÿè®¡:")
    for category, count in sorted(report['statistics']['by_category'].items(), 
                                  key=lambda x: x[1], reverse=True):
        print(f"  {category}: {count} ç¯‡")

if __name__ == '__main__':
    main()
